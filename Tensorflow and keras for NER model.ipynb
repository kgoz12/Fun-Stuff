{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\Java\\jdk1.8.0_221\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install tensorflow_datasets --trusted-host pypi.org --trusted-host files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pyspark\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, RegexTokenizer, StopWordsRemover\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.embeddings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SparkSession \n",
    "spark = sparknlp.start()\n",
    "\n",
    "# import csv\n",
    "df = spark.read.format(\"csv\").options(header='true', inferschema='true', wholeFile='true', multiline='true', escape='\"').load(\"C:\\\\Users\\\\kgoznikar\\\\Desktop\\\\Python\\\\OKC\\\\profiles.csv\")\n",
    "\n",
    "# replace null observations in the essay0 field with blanks\n",
    "def null_as_blank(x):\n",
    "    return when(col(x) != \"\", col(x)).otherwise(\" \")\n",
    "df = df.withColumn(\"essay0\", null_as_blank(\"essay0\"))\n",
    "\n",
    "# add the \"not_working\" column to the dataframe\n",
    "not_working = expr(\n",
    "    \"\"\"IF(job == \"student\" OR job == \"unemployed\" OR job == \"retired\", 1, 0)\"\"\"\n",
    ")\n",
    "df = df.withColumn(\"not_working\", not_working)\n",
    "\n",
    "# use regex to clean unwanted characters from the data\n",
    "df = df.withColumn('essay0Clean', regexp_replace(\"essay0\", \"\\\\n|&nbsp;|<[^>]*>|[^A-za-z|']\", \" \"))\n",
    "\n",
    "# Tokenizing\n",
    "tokenizer = RegexTokenizer(inputCol='essay0Clean', outputCol=\"essay0Tokenzd\", pattern=\"\\\\W\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# Remove stop words\n",
    "stopRemover = StopWordsRemover(inputCol=\"essay0Tokenzd\", outputCol=\"essay0NoStops\")\n",
    "df = stopRemover.transform(df)\n",
    "\n",
    "# turn word lists into a vector of counts\n",
    "vectorizeCounts = CountVectorizer(inputCol=\"essay0NoStops\", outputCol=\"essay0Vector\", vocabSize=200, minDF=3)\n",
    "\n",
    "# save the model so I can pull the vocab from it later\n",
    "modelVec = vectorizeCounts.fit(df)\n",
    "\n",
    "df = modelVec.transform(df) #.transform() is super important - converts result back to a dataframe!!\n",
    "\n",
    "# the field \"essay0Vector\" contains a vector with the ID number of each word. \n",
    "# Can this be passed into keras? how does keras know which ID symbolizes what word??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you like the model you can save it and use it again\n",
    "# model.save(\"/tmp/count_vec_model\")\n",
    "# same_model = CountVectorizerModel.load(\"/tmp/count_vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m', 'love', 'like', 'life', 'people', 'time', 'new', 'good', 'things', 'friends', 'enjoy', 'also', 've', 'get', 'know', 'looking', 'really', 'someone', 'one', 'fun', 'going', 'work', 'go', 'years', 'much', 'music', 'person', 'want', 'think', 'make', 'always', 'well', 'back', 'find', 'pretty', 'live', 'great', 'bay', 'lot', 'try', 'around', 'world', 'san', 'way', 'family', 'area', 'city', 'say', 'meet', 'see', 'open', 'take', 'something', 'moved', 'guy', 'll', 'day', 'francisco', 'travel', 'little', 'laugh', 'd', 'anything', 'still', 'feel', 'working', 'long', 'many', 'trying', 'kind', 'living', 'right', 'sf', 'play', 'year', 'best', 'even', 'food', 'never', 'school', 're', 'home', 'sometimes', 'humor', 'sense', 'hard', 'bit', 'amp', 'happy', 'though', 'big', 'lived', 'movies', 'easy', 'art', 'believe', 'first', 'every', 'hiking', 'im', 'important', 'thing', 'old', 'self', 'man', 'girl', 'better', 'two', 'relationship', 'need', 'nice', 'funny', 'places', 'let', 'everything', 'getting', 'traveling', 'times', 'spend', 'honest', 'grew', 'passionate', 'born', 'keep', 'outdoors', 'job', 'playing', 'others', 'making', 'often', 'reading', 'read', 'heart', 'loving', 'watching', 'college', 'part', 'recently', 'active', 'sports', 'games', 'favorite', 'share', 'got', 'probably', 'since', 'friend', 'learning', 'different', 'mind', 'last', 'interested', 'place', 'talk', 'come', 'meeting', 'free', 'creative', 'interesting', 'exploring', 'ago', 'woman', 'look', 'cooking', 'especially', 'california', 'dance', 'raised', 'dancing', 'wine', 'east', 'yet', 'stuff', 'usually', 'social', 'profile', 'learn', 'enough', 'name', 'maybe', 'ever', 'else', 'far', 'conversation', 'makes', 'care', 'nature', 'smart', 'coast', 'currently', 'tend', 'along', 'may', 'night', 'give', 'adventure', 'us', 'tell', 'serious', 'might']\n"
     ]
    }
   ],
   "source": [
    "# print the first value in essay0Vector column\n",
    "# df.select('essay0Vector').first()\n",
    "\n",
    "\n",
    "# dir(df.select('essay0Vector'))\n",
    "# print the vocabulary in the model object\n",
    "print(modelVec.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we represent each word with an ID number, this will convert that representation into an embedding vector for each word \n",
    "# (where word similarities are captured by the numeric values in the vector)\n",
    "# spark nlp will also perform word embeddings transformation\n",
    "# This layer can only be used as the first layer in a model.\n",
    "\n",
    "# embedding_layer = layers.Embedding(1000, 5)\n",
    "\n",
    "# # result = embedding_layer(tf.constant([1,2,3]))\n",
    "# result = embedding_layer(df['essay0Vector'])\n",
    "# result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE ALL KERAS MODELING RESULTS REPRODUCIBLE...\n",
    "\n",
    "# Seed value\n",
    "seed_value= 123\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# # 5. Configure a new global `tensorflow` session\n",
    "# from tensorflow.keras import backend as K\n",
    "# session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "# K.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0.]\n",
      " [1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0.]\n",
      " [1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.]\n",
      " [1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0.]\n",
      " [0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0.]\n",
      " [0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
      " [1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1.]]\n",
      "[[[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]]\n",
      "[[0 1 1 1 1 0 1 1 1 1 1 1 0 0]\n",
      " [1 0 0 1 0 1 1 1 1 1 1 0 1 1]\n",
      " [1 0 0 0 1 1 0 1 0 0 1 0 1 1]\n",
      " [0 0 0 0 1 1 1 1 1 0 1 0 1 1]\n",
      " [1 0 0 1 0 0 1 0 0 0 0 0 1 1]\n",
      " [0 1 0 0 0 1 1 0 1 0 1 0 1 1]\n",
      " [0 0 0 1 0 1 1 0 0 1 0 0 1 1]\n",
      " [0 0 1 1 0 1 0 1 1 1 0 1 1 1]\n",
      " [1 1 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 0 0 1 1 1 0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 1 0 1 0 0 0]\n",
      " [0 1 1 0 1 0 1 0 1 1 1 0 1 0]\n",
      " [0 0 0 1 1 1 0 0 0 1 1 1 0 0]\n",
      " [0 0 1 0 1 1 0 1 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 1 0 0 1 1 1 1]\n",
      " [1 1 0 0 1 0 1 1 0 1 0 1 0 0]\n",
      " [1 1 1 0 0 1 1 1 0 0 1 1 1 1]\n",
      " [1 0 1 0 0 0 1 0 1 1 1 0 0 0]\n",
      " [1 0 1 1 0 1 0 0 0 0 0 0 1 1]\n",
      " [0 1 1 1 1 1 0 0 1 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# create a toy matrix of zeros and ones to pipe thought 1D CNN tensor flow...test a 1xN kernel\n",
    "onesAndZeros = np.random.randint(2, size = (20, 15)).astype('float32')\n",
    "print(onesAndZeros)\n",
    "\n",
    "# so this is really stupid reshaping...\n",
    "X = np.expand_dims(onesAndZeros, axis=2)\n",
    "\n",
    "print(X)\n",
    "\n",
    "# make dv\n",
    "y = np.random.randint(2, size=(20, 14))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "# if I want to perform predictions on this array then I need to convert each of these elements to acceptable datatypes: float16, bfloat16, float32, float64\n",
    "print(X[0][:][:].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 14, 1)             2         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 14)                0         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# pipe above through 1D CNN \n",
    "\n",
    "# create and compile the model\n",
    "model = Sequential()\n",
    "model.add(keras.layers.Conv1D(input_shape=(15, 1),\n",
    "                    filters=1, \n",
    "                    kernel_size = 2, \n",
    "                    strides=1, \n",
    "                    padding='valid', \n",
    "                    activation=None, \n",
    "                    use_bias=False, \n",
    "                    bias_initializer='zeros'))\n",
    "model.add(keras.layers.Flatten())\n",
    "# model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20 samples\n",
      "20/20 [==============================] - ETA: 7s - loss: 4.8999 - accuracy: 0.42 - 0s 20ms/sample - loss: 6.3995 - accuracy: 0.4714\n"
     ]
    }
   ],
   "source": [
    "# Train the model, iterating on the data in batches of 1 sample - so not interating\n",
    "result = model.fit(X, y, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.47142857\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(model.metrics_names[1], scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[-0.9157342 ]],\n",
      "\n",
      "       [[ 0.26331082]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# dir(model.layers[0])\n",
    "print(model.layers[0].get_weights())\n",
    "weights = model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# do some math...\n",
    "# slide a matrix window of dimension [1 x 2] across the input matrix. Multiply by the above weights matrix of dimension [2 x 1] the resulting matrix should be [20 x 14]\n",
    "\n",
    "# convert array to pandas dataframe to make this task easier.\n",
    "pdOnesAndZeros = pd.DataFrame(onesAndZeros)\n",
    "\n",
    "# convert the weights to a dataframe\n",
    "b = pd.DataFrame(np.asarray(weights).reshape((2,1)))\n",
    "\n",
    "# create an empty matrix in which the dot products are stored\n",
    "matrix = np.zeros((20, 14))\n",
    "\n",
    "# using a stride of 1 slide a window equal to a [1x2] matrix across the onesAndZeros dataframe\n",
    "for i in pdOnesAndZeros.index:\n",
    "    for j in range(2, 16, 1): \n",
    "#         print('i: ', i, 'j: ', j) #confirm the index values are correct\n",
    "        a = pd.DataFrame(pdOnesAndZeros.iloc[i,(j-2):j]).transpose().reset_index(drop=True)\n",
    "        a.columns = range(a.shape[1]) # had to rename to reset the column indexes\n",
    "#         print(a.dot(b)) #these match the output shown below.\n",
    "        matrix[i,(j-2)] = a.dot(b).values # store values in null matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.26331082 -0.91573417  0.          0.          0.          0.\n",
      "   0.26331082 -0.65242338 -0.91573417  0.26331082 -0.65242338 -0.91573417\n",
      "   0.26331082 -0.91573417]\n",
      " [-0.91573417  0.26331082 -0.65242338 -0.91573417  0.          0.\n",
      "   0.26331082 -0.65242338 -0.65242338 -0.91573417  0.26331082 -0.91573417\n",
      "   0.          0.        ]\n",
      " [ 0.26331082 -0.65242338 -0.65242338 -0.91573417  0.          0.26331082\n",
      "  -0.91573417  0.          0.26331082 -0.91573417  0.26331082 -0.91573417\n",
      "   0.26331082 -0.65242338]\n",
      " [-0.91573417  0.          0.          0.          0.26331082 -0.65242338\n",
      "  -0.91573417  0.          0.26331082 -0.91573417  0.26331082 -0.91573417\n",
      "   0.          0.26331082]\n",
      " [ 0.26331082 -0.91573417  0.26331082 -0.91573417  0.          0.\n",
      "   0.          0.26331082 -0.91573417  0.          0.26331082 -0.65242338\n",
      "  -0.65242338 -0.65242338]\n",
      " [ 0.          0.          0.          0.26331082 -0.65242338 -0.91573417\n",
      "   0.          0.          0.          0.26331082 -0.91573417  0.26331082\n",
      "  -0.65242338 -0.91573417]\n",
      " [-0.65242338 -0.65242338 -0.91573417  0.26331082 -0.65242338 -0.65242338\n",
      "  -0.91573417  0.          0.          0.26331082 -0.91573417  0.\n",
      "   0.26331082 -0.65242338]\n",
      " [-0.91573417  0.26331082 -0.65242338 -0.91573417  0.          0.26331082\n",
      "  -0.65242338 -0.91573417  0.26331082 -0.65242338 -0.65242338 -0.65242338\n",
      "  -0.65242338 -0.65242338]\n",
      " [-0.91573417  0.26331082 -0.65242338 -0.65242338 -0.65242338 -0.91573417\n",
      "   0.          0.          0.26331082 -0.65242338 -0.91573417  0.\n",
      "   0.26331082 -0.65242338]\n",
      " [ 0.          0.          0.26331082 -0.91573417  0.26331082 -0.65242338\n",
      "  -0.91573417  0.          0.26331082 -0.91573417  0.26331082 -0.65242338\n",
      "  -0.65242338 -0.65242338]\n",
      " [ 0.26331082 -0.91573417  0.26331082 -0.91573417  0.          0.26331082\n",
      "  -0.65242338 -0.91573417  0.26331082 -0.65242338 -0.65242338 -0.91573417\n",
      "   0.26331082 -0.65242338]\n",
      " [-0.65242338 -0.91573417  0.          0.26331082 -0.65242338 -0.91573417\n",
      "   0.          0.26331082 -0.91573417  0.          0.          0.26331082\n",
      "  -0.91573417  0.26331082]\n",
      " [-0.91573417  0.          0.          0.26331082 -0.91573417  0.\n",
      "   0.26331082 -0.91573417  0.          0.26331082 -0.91573417  0.\n",
      "   0.26331082 -0.91573417]\n",
      " [ 0.          0.          0.26331082 -0.91573417  0.26331082 -0.65242338\n",
      "  -0.91573417  0.          0.          0.26331082 -0.91573417  0.\n",
      "   0.          0.26331082]\n",
      " [ 0.          0.          0.26331082 -0.91573417  0.26331082 -0.65242338\n",
      "  -0.65242338 -0.65242338 -0.91573417  0.26331082 -0.65242338 -0.65242338\n",
      "  -0.65242338 -0.91573417]\n",
      " [ 0.26331082 -0.65242338 -0.65242338 -0.65242338 -0.65242338 -0.91573417\n",
      "   0.          0.26331082 -0.65242338 -0.65242338 -0.91573417  0.26331082\n",
      "  -0.91573417  0.        ]\n",
      " [ 0.26331082 -0.65242338 -0.65242338 -0.91573417  0.26331082 -0.91573417\n",
      "   0.          0.26331082 -0.65242338 -0.65242338 -0.91573417  0.26331082\n",
      "  -0.65242338 -0.91573417]\n",
      " [ 0.          0.26331082 -0.91573417  0.26331082 -0.65242338 -0.91573417\n",
      "   0.          0.26331082 -0.65242338 -0.65242338 -0.65242338 -0.65242338\n",
      "  -0.91573417  0.26331082]\n",
      " [-0.65242338 -0.91573417  0.26331082 -0.65242338 -0.91573417  0.\n",
      "   0.          0.26331082 -0.65242338 -0.91573417  0.          0.26331082\n",
      "  -0.91573417  0.        ]\n",
      " [ 0.26331082 -0.91573417  0.26331082 -0.65242338 -0.65242338 -0.91573417\n",
      "   0.26331082 -0.65242338 -0.65242338 -0.91573417  0.          0.\n",
      "   0.          0.26331082]]\n"
     ]
    }
   ],
   "source": [
    "# print matrix with stored values\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get predictions for each obs\n",
    "preds = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 14)\n",
      "[[ 0.26331082 -0.9157342   0.          0.          0.          0.\n",
      "   0.26331082 -0.6524234  -0.9157342   0.26331082 -0.6524234  -0.9157342\n",
      "   0.26331082 -0.9157342 ]\n",
      " [-0.9157342   0.26331082 -0.6524234  -0.9157342   0.          0.\n",
      "   0.26331082 -0.6524234  -0.6524234  -0.9157342   0.26331082 -0.9157342\n",
      "   0.          0.        ]\n",
      " [ 0.26331082 -0.6524234  -0.6524234  -0.9157342   0.          0.26331082\n",
      "  -0.9157342   0.          0.26331082 -0.9157342   0.26331082 -0.9157342\n",
      "   0.26331082 -0.6524234 ]\n",
      " [-0.9157342   0.          0.          0.          0.26331082 -0.6524234\n",
      "  -0.9157342   0.          0.26331082 -0.9157342   0.26331082 -0.9157342\n",
      "   0.          0.26331082]\n",
      " [ 0.26331082 -0.9157342   0.26331082 -0.9157342   0.          0.\n",
      "   0.          0.26331082 -0.9157342   0.          0.26331082 -0.6524234\n",
      "  -0.6524234  -0.6524234 ]\n",
      " [ 0.          0.          0.          0.26331082 -0.6524234  -0.9157342\n",
      "   0.          0.          0.          0.26331082 -0.9157342   0.26331082\n",
      "  -0.6524234  -0.9157342 ]\n",
      " [-0.6524234  -0.6524234  -0.9157342   0.26331082 -0.6524234  -0.6524234\n",
      "  -0.9157342   0.          0.          0.26331082 -0.9157342   0.\n",
      "   0.26331082 -0.6524234 ]\n",
      " [-0.9157342   0.26331082 -0.6524234  -0.9157342   0.          0.26331082\n",
      "  -0.6524234  -0.9157342   0.26331082 -0.6524234  -0.6524234  -0.6524234\n",
      "  -0.6524234  -0.6524234 ]\n",
      " [-0.9157342   0.26331082 -0.6524234  -0.6524234  -0.6524234  -0.9157342\n",
      "   0.          0.          0.26331082 -0.6524234  -0.9157342   0.\n",
      "   0.26331082 -0.6524234 ]\n",
      " [ 0.          0.          0.26331082 -0.9157342   0.26331082 -0.6524234\n",
      "  -0.9157342   0.          0.26331082 -0.9157342   0.26331082 -0.6524234\n",
      "  -0.6524234  -0.6524234 ]\n",
      " [ 0.26331082 -0.9157342   0.26331082 -0.9157342   0.          0.26331082\n",
      "  -0.6524234  -0.9157342   0.26331082 -0.6524234  -0.6524234  -0.9157342\n",
      "   0.26331082 -0.6524234 ]\n",
      " [-0.6524234  -0.9157342   0.          0.26331082 -0.6524234  -0.9157342\n",
      "   0.          0.26331082 -0.9157342   0.          0.          0.26331082\n",
      "  -0.9157342   0.26331082]\n",
      " [-0.9157342   0.          0.          0.26331082 -0.9157342   0.\n",
      "   0.26331082 -0.9157342   0.          0.26331082 -0.9157342   0.\n",
      "   0.26331082 -0.9157342 ]\n",
      " [ 0.          0.          0.26331082 -0.9157342   0.26331082 -0.6524234\n",
      "  -0.9157342   0.          0.          0.26331082 -0.9157342   0.\n",
      "   0.          0.26331082]\n",
      " [ 0.          0.          0.26331082 -0.9157342   0.26331082 -0.6524234\n",
      "  -0.6524234  -0.6524234  -0.9157342   0.26331082 -0.6524234  -0.6524234\n",
      "  -0.6524234  -0.9157342 ]\n",
      " [ 0.26331082 -0.6524234  -0.6524234  -0.6524234  -0.6524234  -0.9157342\n",
      "   0.          0.26331082 -0.6524234  -0.6524234  -0.9157342   0.26331082\n",
      "  -0.9157342   0.        ]\n",
      " [ 0.26331082 -0.6524234  -0.6524234  -0.9157342   0.26331082 -0.9157342\n",
      "   0.          0.26331082 -0.6524234  -0.6524234  -0.9157342   0.26331082\n",
      "  -0.6524234  -0.9157342 ]\n",
      " [ 0.          0.26331082 -0.9157342   0.26331082 -0.6524234  -0.9157342\n",
      "   0.          0.26331082 -0.6524234  -0.6524234  -0.6524234  -0.6524234\n",
      "  -0.9157342   0.26331082]\n",
      " [-0.6524234  -0.9157342   0.26331082 -0.6524234  -0.9157342   0.\n",
      "   0.          0.26331082 -0.6524234  -0.9157342   0.          0.26331082\n",
      "  -0.9157342   0.        ]\n",
      " [ 0.26331082 -0.9157342   0.26331082 -0.6524234  -0.6524234  -0.9157342\n",
      "   0.26331082 -0.6524234  -0.6524234  -0.9157342   0.          0.\n",
      "   0.          0.26331082]]\n"
     ]
    }
   ],
   "source": [
    "# good, output is of correct dimensionality\n",
    "print(preds.shape)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# check for any differences between \"hand calc\" matrix and model predictions\n",
    "print(matrix - preds)\n",
    "\n",
    "# :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
